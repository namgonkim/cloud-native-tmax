# 클라우드 개발자 양성과정

## 상용 클라우드를 이용한 어플리케이션 배포
[소스코드](https://github.com/namgonkim/msa-ecommerce-tmax)
* 토이 프로젝트를 통한 msa 어플리케이션 실습
    - 데이터 동기화 문제 해결을 위한 kafka의 EcoSystem


## EcoSystem
* 카프카 connector에 사용할 마리아 DB 생성 및 세팅

### mariaDB
```
mariadb -h127.0.0.1 -uroot -p
-> Enter password
show databases;
use mydb;
show tables;
select * from orders;
select * from users;
```

### connector
* 카프카 커넥터가 실행되기 위해서는 카프카 클러스터가 필요하다.
    - 카프카 클러스터: 주키퍼 + 카프카 브로커


### 현재까지 진행 상황
1. zookeeper
2. kafka server
3. mariadb x 2
    - src 원본 => mydb (users, orders)
    - target 타겟 => mydb_backup (x)
4. kafka connector
5. kafka JDBC connector (plugin 연결)
6. http://localhost:8083/connector_plugins 로 1~5 작업이 완료 되었는지 확인
    - JDBCSourceConnector, JDBCSinkConnector가 보이면 성공

### To-do
1. mariaDB driver 설치
2. Source Connector 스크립트 작성
    * Source Connector : 원본 데이터를 사용하고 있는 카프카의 토픽으로 전달
        - postman으로 source connector 스크립트를 작성해 보냈다.
3. Sink connector 스크립트 작성
4. Demo Test


#### kafka connector 실습
```
* Local
    - kafka cluster
    - kafka connector
mydb --> source connector --> kafka connector
가 현재까지의 상황이었다.
```
1. sink connect를 새롭게 생성.
    - 스크립트 새로 작성.
    - 이 스크립트를 aws의 rds로 옮기는 작업을 진행한다.
2. local pc의 mydb를 aws rds로 복사하는 작업.

> 보안 이슈로 노션 링크는 생략하겠습니다. 확인이 필요하면 __zxcv9455@naver.com__으로 연락바랍니다.